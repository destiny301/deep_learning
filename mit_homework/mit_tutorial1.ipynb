{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mit_tutorial1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1zn1E1X3BV5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "92625af3-afa6-4101-e39f-449b02406f3d"
      },
      "source": [
        "%%bash\n",
        "rm -rf 6864-hw1\n",
        "git clone https://github.com/lingo-mit/6864-hw1.git"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into '6864-hw1'...\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "is5Xdd3_4UVF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.append('/content/6864-hw1')\n",
        "\n",
        "import numpy as np\n",
        "import csv\n",
        "import itertools as it\n",
        "np.random.seed(0)\n",
        "\n",
        "import lab_util"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohj82B1S6Baz",
        "colab_type": "text"
      },
      "source": [
        "# 01.dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3-Hka21D6AXJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "outputId": "8d449208-7100-466d-cb10-b8754eba9187"
      },
      "source": [
        "data = []\n",
        "num_positive = 0 #data balance\n",
        "num_display = 0 #for checking\n",
        "\n",
        "with open('/content/6864-hw1/reviews.csv') as reader:\n",
        "  datareader = csv.reader(reader)\n",
        "  next(datareader)\n",
        "\n",
        "  for id, review, label in datareader:\n",
        "    label = int(label)\n",
        "\n",
        "    if label == 1:\n",
        "      if num_positive == 2000:\n",
        "        continue\n",
        "      num_positive += 1\n",
        "\n",
        "    if len(data) == 4000:\n",
        "      break\n",
        "\n",
        "    data.append((review, label))\n",
        "\n",
        "    #display the first 5 data to check\n",
        "    if num_display > 5:\n",
        "      continue\n",
        "    \n",
        "    num_display += 1\n",
        "    print(\"--------------------\")\n",
        "    print(\"review:\", review)\n",
        "    print(\"label:\", label)\n",
        "  \n",
        "\n",
        "  print(\"total num of reviews:\", len(data))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--------------------\n",
            "review: I have bought several of the Vitality canned dog food products and have found them all to be of good quality. The product looks more like a stew than a processed meat and it smells better. My Labrador is finicky and she appreciates this product better than  most.\n",
            "label: 1\n",
            "--------------------\n",
            "review: Product arrived labeled as Jumbo Salted Peanuts...the peanuts were actually small sized unsalted. Not sure if this was an error or if the vendor intended to represent the product as \"Jumbo\".\n",
            "label: 0\n",
            "--------------------\n",
            "review: This is a confection that has been around a few centuries.  It is a light, pillowy citrus gelatin with nuts - in this case Filberts. And it is cut into tiny squares and then liberally coated with powdered sugar.  And it is a tiny mouthful of heaven.  Not too chewy, and very flavorful.  I highly recommend this yummy treat.  If you are familiar with the story of C.S. Lewis' \"The Lion, The Witch, and The Wardrobe\" - this is the treat that seduces Edmund into selling out his Brother and Sisters to the Witch.\n",
            "label: 1\n",
            "--------------------\n",
            "review: If you are looking for the secret ingredient in Robitussin I believe I have found it.  I got this in addition to the Root Beer Extract I ordered (which was good) and made some cherry soda.  The flavor is very medicinal.\n",
            "label: 0\n",
            "--------------------\n",
            "review: Great taffy at a great price.  There was a wide assortment of yummy taffy.  Delivery was very quick.  If your a taffy lover, this is a deal.\n",
            "label: 1\n",
            "--------------------\n",
            "review: I got a wild hair for taffy and ordered this five pound bag. The taffy was all very enjoyable with many flavors: watermelon, root beer, melon, peppermint, grape, etc. My only complaint is there was a bit too much red/black licorice-flavored pieces (just not my particular favorites). Between me, my kids, and my husband, this lasted only two weeks! I would recommend this brand of taffy -- it was a delightful treat.\n",
            "label: 1\n",
            "total num of reviews: 4000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJP6Nacz5R0l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.shuffle(data)\n",
        "reviews, labels = zip(*data)\n",
        "\n",
        "#Q: test data/train data-->balance?\n",
        "xtr = reviews[:3000]\n",
        "ytr = labels[:3000]\n",
        "xval = reviews[3000:3500]\n",
        "yval = labels[3000:3500]\n",
        "xte = reviews[3500:]\n",
        "yte = labels[3500:]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPzlW_zq_Ehb",
        "colab_type": "text"
      },
      "source": [
        "# 02. word representation(method01. matrix factorization)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tUIqoDO79lVJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c26564e0-7c36-4f80-b16b-209888415134"
      },
      "source": [
        "vectorizer = lab_util.CountVectorizer()\n",
        "vectorizer.fit(xtr)\n",
        "td_matrix = vectorizer.transform(xtr).T\n",
        "print(f\"TD matrix is {td_matrix.shape[0]} x {td_matrix.shape[1]}\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TD matrix is 2006 x 3000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_sPtWFhXD8JN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.decomposition import TruncatedSVD"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hXTWkaIr_gNs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def learn_reps_lsa(matrix, rep_size):\n",
        "  # `matrix` is a `|V| x n` matrix, where `|V|` is the number of words in the\n",
        "  # vocabulary. This function should return a `|V| x rep_size` matrix with each\n",
        "  # row corresponding to a word representation. The `sklearn.decomposition` \n",
        "  # package may be useful.\n",
        "\n",
        "  # Your code here!\n",
        "  svd_model = TruncatedSVD(n_components=rep_size)\n",
        "  lsa = svd_model.fit_transform(matrix)\n",
        "\n",
        "  return lsa"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfKhwHGyFdXW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 745
        },
        "outputId": "402fc44c-cdbd-4b18-f774-a219cec18862"
      },
      "source": [
        "reps = learn_reps_lsa(td_matrix, 500)\n",
        "words = [\"good\", \"bad\", \"cookie\", \"jelly\", \"dog\", \"the\", \"4\"]\n",
        "show_tokens = [vectorizer.tokenizer.word_to_token[word] for word in words]\n",
        "lab_util.show_similar_words(vectorizer.tokenizer, reps, show_tokens)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "good 47\n",
            "  . 1.056\n",
            "  a 1.101\n",
            "  but 1.121\n",
            "  , 1.152\n",
            "  the 1.157\n",
            "bad 201\n",
            "  . 1.396\n",
            "  taste 1.416\n",
            "  but 1.434\n",
            "  a 1.435\n",
            "  i 1.449\n",
            "cookie 504\n",
            "  nana's 0.775\n",
            "  cookies 1.012\n",
            "  oreos 1.283\n",
            "  bars 1.359\n",
            "  bites 1.380\n",
            "jelly 351\n",
            "  twist 1.160\n",
            "  cardboard 1.259\n",
            "  plastic 1.427\n",
            "  advertised 1.428\n",
            "  peanuts 1.461\n",
            "dog 925\n",
            "  food 1.049\n",
            "  pets 1.071\n",
            "  pet 1.072\n",
            "  switched 1.206\n",
            "  foods 1.228\n",
            "the 36\n",
            "  . 0.331\n",
            "  <unk> 0.366\n",
            "  of 0.395\n",
            "  and 0.403\n",
            "  to 0.422\n",
            "4 292\n",
            "  1 1.048\n",
            "  6 1.118\n",
            "  70 1.132\n",
            "  stevia 1.196\n",
            "  concentrated 1.240\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0iyr3xoGwIe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfTransformer"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPrc9QTVFhrY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def transform_tfidf(matrix):\n",
        "  # `matrix` is a `|V| x |D|` matrix of raw counts, where `|V|` is the \n",
        "  # vocabulary size and `|D|` is the number of documents in the corpus. This\n",
        "  # function should (nondestructively) return a version of `matrix` with the\n",
        "  # TF-IDF transform appliied.\n",
        "\n",
        "  # Your code here!\n",
        "  tf_idf = TfidfTransformer()\n",
        "  return tf_idf.fit_transform(matrix)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CKgvxfnwGSQi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 745
        },
        "outputId": "b05cc304-5061-4c6d-b814-7c1c1e8f0d2f"
      },
      "source": [
        "td_matrix_tfidf = transform_tfidf(td_matrix)\n",
        "reps_tfidf = learn_reps_lsa(td_matrix_tfidf, 500)\n",
        "lab_util.show_similar_words(vectorizer.tokenizer, reps_tfidf, show_tokens)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "good 47\n",
            "  . 0.835\n",
            "  but 0.842\n",
            "  a 0.856\n",
            "  is 0.906\n",
            "  and 0.911\n",
            "bad 201\n",
            "  taste 1.190\n",
            "  like 1.228\n",
            "  but 1.251\n",
            "  . 1.267\n",
            "  a 1.294\n",
            "cookie 504\n",
            "  nana's 0.713\n",
            "  cookies 0.955\n",
            "  bars 1.276\n",
            "  oreos 1.458\n",
            "  gluten 1.467\n",
            "jelly 351\n",
            "  cardboard 1.223\n",
            "  twist 1.239\n",
            "  softer 1.351\n",
            "  plum 1.413\n",
            "  supermarket 1.489\n",
            "dog 925\n",
            "  food 0.859\n",
            "  pet 1.039\n",
            "  foods 1.055\n",
            "  dogs 1.100\n",
            "  pets 1.100\n",
            "the 36\n",
            "  . 0.199\n",
            "  and 0.254\n",
            "  of 0.261\n",
            "  <unk> 0.276\n",
            "  to 0.306\n",
            "4 292\n",
            "  1 0.833\n",
            "  6 0.976\n",
            "  2 1.079\n",
            "  stevia 1.091\n",
            "  3 1.113\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r-uiKvP6zNck",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "b37bdfea-28d6-422f-eea6-10b77052d8e7"
      },
      "source": [
        "print(td_matrix.shape)\n",
        "print(reps_tfidf.shape)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2006, 3000)\n",
            "(2006, 500)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKdCkuKVHzSx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def word_featurizer(xs):\n",
        "  # normalize\n",
        "  return xs / np.sqrt((xs ** 2).sum(axis=1, keepdims=True))\n",
        "\n",
        "def lsa_featurizer(xs):\n",
        "  # This function takes in a matrix in which each row contains the word counts\n",
        "  # for the given review. It should return a matrix in which each row contains\n",
        "  # the learned feature representation of each review (e.g. the sum of LSA \n",
        "  # word representations).\n",
        "\n",
        "  feats = None # Your code here!\n",
        "  td_matrix_tfidf = transform_tfidf(xs)\n",
        "  feats = learn_reps_lsa(td_matrix_tfidf, 500)\n",
        "  # feats = learn_reps_lsa(td_matrix_tfidf, 500).sum(axis = 1)\n",
        "\n",
        "\n",
        "  # normalize\n",
        "  return feats / np.sqrt((feats ** 2).sum(axis=1, keepdims=True))\n"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KInyaam4vgiJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 537
        },
        "outputId": "ddb2f728-6b4d-4441-9fc8-849e9f706885"
      },
      "source": [
        "\n",
        "def combo_featurizer(xs):\n",
        "  return np.concatenate((word_featurizer(xs), lsa_featurizer(xs)), axis=1)\n",
        "\n",
        "def train_model(featurizer, xs, ys):\n",
        "  import sklearn.linear_model\n",
        "  xs_featurized = featurizer(xs)\n",
        "  model = sklearn.linear_model.LogisticRegression()\n",
        "  model.fit(xs_featurized, ys)\n",
        "  return model\n",
        "\n",
        "def eval_model(model, featurizer, xs, ys):\n",
        "  xs_featurized = featurizer(xs)\n",
        "  pred_ys = model.predict(xs_featurized)\n",
        "  print(\"test accuracy\", np.mean(pred_ys == ys))\n",
        "\n",
        "def training_experiment(name, featurizer, n_train):\n",
        "  print(f\"{name} features, {n_train} examples\")\n",
        "  train_xs = vectorizer.transform(xtr[:n_train])\n",
        "  train_ys = ytr[:n_train]\n",
        "  test_xs = vectorizer.transform(xte)\n",
        "  test_ys = yte\n",
        "  model = train_model(featurizer, train_xs, train_ys)\n",
        "  eval_model(model, featurizer, test_xs, test_ys)\n",
        "  print()\n",
        "\n",
        "training_experiment(\"word\", word_featurizer, 10)\n",
        "training_experiment(\"lsa\", lsa_featurizer, 10)\n",
        "training_experiment(\"combo\", combo_featurizer, 10)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "word features, 10 examples\n",
            "test accuracy 0.496\n",
            "\n",
            "lsa features, 10 examples\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-39-62cc3bbb17b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mtraining_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"word\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_featurizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mtraining_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"lsa\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlsa_featurizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0mtraining_experiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"combo\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcombo_featurizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-39-62cc3bbb17b6>\u001b[0m in \u001b[0;36mtraining_experiment\u001b[0;34m(name, featurizer, n_train)\u001b[0m\n\u001b[1;32m     21\u001b[0m   \u001b[0mtest_xs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxte\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m   \u001b[0mtest_ys\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0myte\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m   \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeaturizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_xs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_ys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m   \u001b[0meval_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeaturizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_xs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_ys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-39-62cc3bbb17b6>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(featurizer, xs, ys)\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0mxs_featurized\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeaturizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m   \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m   \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs_featurized\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/_logistic.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1525\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m         X, y = check_X_y(X, y, accept_sparse='csr', dtype=_dtype, order=\"C\",\n\u001b[0;32m-> 1527\u001b[0;31m                          accept_large_sparse=solver != 'liblinear')\n\u001b[0m\u001b[1;32m   1528\u001b[0m         \u001b[0mcheck_classification_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    753\u001b[0m                     \u001b[0mensure_min_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_min_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m                     \u001b[0mwarn_on_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwarn_on_dtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 755\u001b[0;31m                     estimator=estimator)\n\u001b[0m\u001b[1;32m    756\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    554\u001b[0m                     \u001b[0;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m                     \u001b[0;34m\"your data has a single feature or array.reshape(1, -1) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 556\u001b[0;31m                     \"if it contains a single sample.\".format(array))\n\u001b[0m\u001b[1;32m    557\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m         \u001b[0;31m# in the future np.flexible dtypes will be handled like object dtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[ 3.35171064e-01  2.49639366e-04  5.00501899e-01  3.73404469e-01\n  5.36315922e-01  1.27151476e-01  1.35010079e-01 -1.73525348e-01\n  3.23774005e-01 -2.01882656e-01].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MS7LcVZg2MRW",
        "colab_type": "text"
      },
      "source": [
        "# method02. via language modeling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o7QCtrf4zdBe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as torch_data\n",
        "\n",
        "class Word2VecModel(nn.Module):\n",
        "  # A torch module implementing a word2vec predictor. The `forward` function\n",
        "  # should take a batch of context word ids as input and predict the word \n",
        "  # in the middle of the context as output, as in the CBOW model from lecture.\n",
        "\n",
        "  def __init__(self, vocab_size, embed_dim):\n",
        "      super().__init__()\n",
        "\n",
        "      # Your code here!\n",
        "\n",
        "  def forward(self, context):\n",
        "      # Context is an `n_batch x n_context` matrix of integer word ids\n",
        "      # this function should return a set of scores for predicting the word \n",
        "      # in the middle of the context\n",
        "\n",
        "      # Your code here!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fGeeS3J62XB9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def learn_reps_word2vec(corpus, window_size, rep_size, n_epochs, n_batch):\n",
        "  # This method takes in a corpus of training sentences. It returns a matrix of\n",
        "  # word embeddings with the same structure as used in the previous section of \n",
        "  # the assignment. (You can extract this matrix from the parameters of the \n",
        "  # Word2VecModel.)\n",
        "\n",
        "  tokenizer = lab_util.Tokenizer()\n",
        "  tokenizer.fit(corpus)\n",
        "  tokenized_corpus = tokenizer.tokenize(corpus)\n",
        "\n",
        "  ngrams = lab_util.get_ngrams(tokenized_corpus, window_size)\n",
        "\n",
        "  device = torch.device('cuda')  # run on colab gpu\n",
        "  model = Word2VecModel(tokenizer.vocab_size, rep_size).to(device)\n",
        "  opt = optim.Adam(model.parameters(), lr=0.001)\n",
        "  loss_fn = None # Your code here\n",
        "\n",
        "  loader = torch_data.DataLoader(ngrams, batch_size=n_batch, shuffle=True)\n",
        "\n",
        "  for epoch in range(n_epochs):\n",
        "    for context, label in loader:\n",
        "      # as described above, `context` is a batch of context word ids, and\n",
        "      # `label` is a batch of predicted word labels\n",
        "      pass\n",
        "      # Your code here!\n",
        "\n",
        "  # reminder: you want to return a `vocab_size x embedding_size` numpy array\n",
        "  embedding_matrix = None\n",
        "  # Your code here!"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xhr18f4S2XHx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "reps_word2vec = learn_reps_word2vec(train_reviews, 2, 500, 10, 100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "muEMB8522XMa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lab_util.show_similar_words(vectorizer.tokenizer, reps_word2vec, show_tokens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6630GVxm2d33",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "indices = KMeans(n_clusters=10).fit_predict(reps_word2vec)\n",
        "zipped = list(zip(range(vectorizer.tokenizer.vocab_size), indices))\n",
        "np.random.shuffle(zipped)\n",
        "zipped = zipped[:100]\n",
        "zipped = sorted(zipped, key=lambda x: x[1])\n",
        "for token, cluster_idx in zipped:\n",
        "  word = vectorizer.tokenizer.token_to_word[token]\n",
        "  print(f\"{word}: {cluster_idx}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "plQVfYj22d61",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lsa_featurizer(xs):\n",
        "  feats = None # Your code here!\n",
        "\n",
        "  # normalize\n",
        "  return feats / np.sqrt((feats ** 2).sum(axis=1, keepdims=True))\n",
        "\n",
        "training_experiment(\"word2vec\", lsa_featurizer, 10)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}